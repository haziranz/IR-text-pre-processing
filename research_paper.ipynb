{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "research paper.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOnRv3Ps8cYo3quFOp0RrvF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/haziranz/IR-text-pre-processing/blob/main/research_paper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UqN1i6h5ke4F"
      },
      "source": [
        "# load data file-reserch paper text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vL4mn3eoPMbp"
      },
      "source": [
        "path = \"/content/research paper.txt\"\n",
        "file1 = open(path, \"r\")\n",
        "text = file1.read()\n"
      ],
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WX_NwFCskpDY"
      },
      "source": [
        "# tokenization \n",
        "campare with many tokenizers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0izGCScNsW30"
      },
      "source": [
        "**Tokenization using Python’s split() function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gEQR1WPGsVb1",
        "outputId": "9fef851e-ec85-4b10-ccf9-f627f10d03ea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(list(iter(text.split())))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Neural', 'network', 'models', 'have', 'shown', 'their', 'promising', 'opportunities', 'for', 'multi-task', 'learning,', 'which', 'focus', 'on', 'learning', 'the', 'shared', 'layers', 'to', 'extract', 'the', 'common', 'and', 'task-invariant', 'features.', 'However,', 'in', 'most', 'existing', 'approaches,', 'the', 'extracted', 'shared', 'features', 'are', 'prone', 'to', 'be', 'contaminated', 'by', 'task-specific', 'features', 'or', 'the', 'noise', 'brought', 'by', 'other', 'tasks.', 'In', 'this', 'paper,', 'we', 'propose', 'an', 'adversarial', 'multi-task', 'learning', 'framework,', 'alleviating', 'the', 'shared', 'and', 'private', 'latent', 'feature', 'spaces', 'from', 'interfering', 'with', 'each', 'other.', 'We', 'conduct', 'extensive', 'experiments', 'on', '16', 'different', 'text', 'classification', 'tasks,', 'which', 'demonstrates', 'the', 'benefits', 'of', 'our', 'approach.', 'Besides,', 'we', 'show', 'that', 'the', 'shared', 'knowledge', 'learned', 'by', 'our', 'proposed', 'model', 'can', 'be', 'regarded', 'as', 'off-the-shelf', 'knowledge', 'and', 'easily', 'transferred', 'to', 'new', 'tasks.', 'Multi-task', 'learning', 'is', 'an', 'effective', 'approach', 'to', 'improve', 'the', 'performance', 'of', 'a', 'single', 'task', 'with', 'the', 'help', 'of', 'other', 'related', 'tasks.', 'Recently,', 'neural-based', 'models', 'for', 'multi-task', 'learning', 'have', 'become', 'very', 'popular,', 'ranging', 'from', 'computer', 'vision', '(Misra', 'et', 'al.,', '2016;', 'Zhang', 'et', 'al.,', '2014)', 'to', 'natural', 'language', 'processing', '(Collobert', 'andWeston,', '2008;', 'Luong', 'et', 'al.,', '2015),', 'since', 'they', 'provide', 'a', 'convenient', 'way', 'of', 'combining', 'information', 'from', 'multiple', 'tasks.', 'However,', 'most', 'existing', 'work', 'on', 'multi-task', 'learning', '(Liu', 'et', 'al.,', '2016c,b)', 'attempts', 'to', 'divide', 'the', 'features', 'of', 'different', 'tasks', 'into', 'private', 'and', 'shared', 'spaces,', 'merely', 'based', 'on', 'whether', 'parameters', 'of', 'some', 'components', 'should', 'be', 'shared.', 'As', 'shown', 'in', 'Figure', '1-(a),', 'the', 'general', 'shared-private', 'model', 'introduces', 'two', 'feature', 'spaces', 'for', 'any', 'task:', 'one', 'is', 'used', 'to', 'store', 'task-dependent', 'features,', 'the', 'other', 'is', 'used', 'to', 'capture', 'shared', 'features.', 'The', 'major', 'limitation', 'of', 'this', 'framework', 'is', 'that', 'the', 'shared', 'feature', 'space', 'could', 'contain', 'some', 'unnecessary', 'task-specific', 'features,', 'while', 'some', 'sharable', 'features', 'could', 'also', 'be', 'mixed', 'in', 'private', 'space,', 'suffering', 'from', 'feature', 'redundancy.', 'Taking', 'the', 'following', 'two', 'sentences', 'as', 'examples,', 'which', 'are', 'extracted', 'from', 'two', 'different', 'sentiment', 'classification', 'tasks:', 'Movie', 'reviews', 'and', 'Baby', 'products', 'reviews.', 'The', 'infantile', 'cart', 'is', 'simple', 'and', 'easy', 'to', 'use.', 'This', 'kind', 'of', 'humour', 'is', 'infantile', 'and', 'boring.', 'The', 'word', '�infantile�', 'indicates', 'negative', 'sentiment', 'in', 'Movie', 'task', 'while', 'it', 'is', 'neutral', 'in', 'Baby', 'task.', 'However,', 'the', 'general', 'shared-private', 'model', 'could', 'place', 'the', 'task-specific', 'word', '�infantile�', 'in', 'a', 'shared', 'space,', 'leaving', 'potential', 'hazards', 'for', 'other', 'tasks.', 'Additionally,', 'the', 'capacity', 'of', 'shared', 'space', 'could', 'also', 'be', 'wasted', 'by', 'some', 'unnecessary', 'features.', 'To', 'address', 'this', 'problem,', 'in', 'this', 'paper', 'we', 'propose', 'an', 'adversarial', 'multi-task', 'framework,', 'in', 'which', 'the', 'shared', 'and', 'private', 'feature', 'spaces', 'are', 'in', 'herently', 'disjoint', 'by', 'introducing', 'orthogonality', 'constraints.Specifically,', 'we', 'design', 'a', 'generic', 'shared', 'private', 'learning', 'framework', 'to', 'model', 'the', 'text', 'sequence.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zqQU_YhuCGu"
      },
      "source": [
        "**Tokenization using Regular Expressions (RegEx)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qxES4AfuEjr",
        "outputId": "280022c1-6380-4f9a-dcc1-50847152da78",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import re\n",
        "tokens = re.findall(\"[\\w']+\", text)\n",
        "print(list(iter(tokens)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Neural', 'network', 'models', 'have', 'shown', 'their', 'promising', 'opportunities', 'for', 'multi', 'task', 'learning', 'which', 'focus', 'on', 'learning', 'the', 'shared', 'layers', 'to', 'extract', 'the', 'common', 'and', 'task', 'invariant', 'features', 'However', 'in', 'most', 'existing', 'approaches', 'the', 'extracted', 'shared', 'features', 'are', 'prone', 'to', 'be', 'contaminated', 'by', 'task', 'specific', 'features', 'or', 'the', 'noise', 'brought', 'by', 'other', 'tasks', 'In', 'this', 'paper', 'we', 'propose', 'an', 'adversarial', 'multi', 'task', 'learning', 'framework', 'alleviating', 'the', 'shared', 'and', 'private', 'latent', 'feature', 'spaces', 'from', 'interfering', 'with', 'each', 'other', 'We', 'conduct', 'extensive', 'experiments', 'on', '16', 'different', 'text', 'classification', 'tasks', 'which', 'demonstrates', 'the', 'benefits', 'of', 'our', 'approach', 'Besides', 'we', 'show', 'that', 'the', 'shared', 'knowledge', 'learned', 'by', 'our', 'proposed', 'model', 'can', 'be', 'regarded', 'as', 'off', 'the', 'shelf', 'knowledge', 'and', 'easily', 'transferred', 'to', 'new', 'tasks', 'Multi', 'task', 'learning', 'is', 'an', 'effective', 'approach', 'to', 'improve', 'the', 'performance', 'of', 'a', 'single', 'task', 'with', 'the', 'help', 'of', 'other', 'related', 'tasks', 'Recently', 'neural', 'based', 'models', 'for', 'multi', 'task', 'learning', 'have', 'become', 'very', 'popular', 'ranging', 'from', 'computer', 'vision', 'Misra', 'et', 'al', '2016', 'Zhang', 'et', 'al', '2014', 'to', 'natural', 'language', 'processing', 'Collobert', 'andWeston', '2008', 'Luong', 'et', 'al', '2015', 'since', 'they', 'provide', 'a', 'convenient', 'way', 'of', 'combining', 'information', 'from', 'multiple', 'tasks', 'However', 'most', 'existing', 'work', 'on', 'multi', 'task', 'learning', 'Liu', 'et', 'al', '2016c', 'b', 'attempts', 'to', 'divide', 'the', 'features', 'of', 'different', 'tasks', 'into', 'private', 'and', 'shared', 'spaces', 'merely', 'based', 'on', 'whether', 'parameters', 'of', 'some', 'components', 'should', 'be', 'shared', 'As', 'shown', 'in', 'Figure', '1', 'a', 'the', 'general', 'shared', 'private', 'model', 'introduces', 'two', 'feature', 'spaces', 'for', 'any', 'task', 'one', 'is', 'used', 'to', 'store', 'task', 'dependent', 'features', 'the', 'other', 'is', 'used', 'to', 'capture', 'shared', 'features', 'The', 'major', 'limitation', 'of', 'this', 'framework', 'is', 'that', 'the', 'shared', 'feature', 'space', 'could', 'contain', 'some', 'unnecessary', 'task', 'specific', 'features', 'while', 'some', 'sharable', 'features', 'could', 'also', 'be', 'mixed', 'in', 'private', 'space', 'suffering', 'from', 'feature', 'redundancy', 'Taking', 'the', 'following', 'two', 'sentences', 'as', 'examples', 'which', 'are', 'extracted', 'from', 'two', 'different', 'sentiment', 'classification', 'tasks', 'Movie', 'reviews', 'and', 'Baby', 'products', 'reviews', 'The', 'infantile', 'cart', 'is', 'simple', 'and', 'easy', 'to', 'use', 'This', 'kind', 'of', 'humour', 'is', 'infantile', 'and', 'boring', 'The', 'word', 'infantile', 'indicates', 'negative', 'sentiment', 'in', 'Movie', 'task', 'while', 'it', 'is', 'neutral', 'in', 'Baby', 'task', 'However', 'the', 'general', 'shared', 'private', 'model', 'could', 'place', 'the', 'task', 'specific', 'word', 'infantile', 'in', 'a', 'shared', 'space', 'leaving', 'potential', 'hazards', 'for', 'other', 'tasks', 'Additionally', 'the', 'capacity', 'of', 'shared', 'space', 'could', 'also', 'be', 'wasted', 'by', 'some', 'unnecessary', 'features', 'To', 'address', 'this', 'problem', 'in', 'this', 'paper', 'we', 'propose', 'an', 'adversarial', 'multi', 'task', 'framework', 'in', 'which', 'the', 'shared', 'and', 'private', 'feature', 'spaces', 'are', 'in', 'herently', 'disjoint', 'by', 'introducing', 'orthogonality', 'constraints', 'Specifically', 'we', 'design', 'a', 'generic', 'shared', 'private', 'learning', 'framework', 'to', 'model', 'the', 'text', 'sequence']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQEmjc15wGA8"
      },
      "source": [
        " **Tokenization using NLTK**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-udzsICvmaf",
        "outputId": "93b60aeb-be7a-4062-e1b2-c412f232697f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from nltk.tokenize import word_tokenize \n",
        "\n",
        "print(list(iter(word_tokenize(text))))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Neural', 'network', 'models', 'have', 'shown', 'their', 'promising', 'opportunities', 'for', 'multi-task', 'learning', ',', 'which', 'focus', 'on', 'learning', 'the', 'shared', 'layers', 'to', 'extract', 'the', 'common', 'and', 'task-invariant', 'features', '.', 'However', ',', 'in', 'most', 'existing', 'approaches', ',', 'the', 'extracted', 'shared', 'features', 'are', 'prone', 'to', 'be', 'contaminated', 'by', 'task-specific', 'features', 'or', 'the', 'noise', 'brought', 'by', 'other', 'tasks', '.', 'In', 'this', 'paper', ',', 'we', 'propose', 'an', 'adversarial', 'multi-task', 'learning', 'framework', ',', 'alleviating', 'the', 'shared', 'and', 'private', 'latent', 'feature', 'spaces', 'from', 'interfering', 'with', 'each', 'other', '.', 'We', 'conduct', 'extensive', 'experiments', 'on', '16', 'different', 'text', 'classification', 'tasks', ',', 'which', 'demonstrates', 'the', 'benefits', 'of', 'our', 'approach', '.', 'Besides', ',', 'we', 'show', 'that', 'the', 'shared', 'knowledge', 'learned', 'by', 'our', 'proposed', 'model', 'can', 'be', 'regarded', 'as', 'off-the-shelf', 'knowledge', 'and', 'easily', 'transferred', 'to', 'new', 'tasks', '.', 'Multi-task', 'learning', 'is', 'an', 'effective', 'approach', 'to', 'improve', 'the', 'performance', 'of', 'a', 'single', 'task', 'with', 'the', 'help', 'of', 'other', 'related', 'tasks', '.', 'Recently', ',', 'neural-based', 'models', 'for', 'multi-task', 'learning', 'have', 'become', 'very', 'popular', ',', 'ranging', 'from', 'computer', 'vision', '(', 'Misra', 'et', 'al.', ',', '2016', ';', 'Zhang', 'et', 'al.', ',', '2014', ')', 'to', 'natural', 'language', 'processing', '(', 'Collobert', 'andWeston', ',', '2008', ';', 'Luong', 'et', 'al.', ',', '2015', ')', ',', 'since', 'they', 'provide', 'a', 'convenient', 'way', 'of', 'combining', 'information', 'from', 'multiple', 'tasks', '.', 'However', ',', 'most', 'existing', 'work', 'on', 'multi-task', 'learning', '(', 'Liu', 'et', 'al.', ',', '2016c', ',', 'b', ')', 'attempts', 'to', 'divide', 'the', 'features', 'of', 'different', 'tasks', 'into', 'private', 'and', 'shared', 'spaces', ',', 'merely', 'based', 'on', 'whether', 'parameters', 'of', 'some', 'components', 'should', 'be', 'shared', '.', 'As', 'shown', 'in', 'Figure', '1-', '(', 'a', ')', ',', 'the', 'general', 'shared-private', 'model', 'introduces', 'two', 'feature', 'spaces', 'for', 'any', 'task', ':', 'one', 'is', 'used', 'to', 'store', 'task-dependent', 'features', ',', 'the', 'other', 'is', 'used', 'to', 'capture', 'shared', 'features', '.', 'The', 'major', 'limitation', 'of', 'this', 'framework', 'is', 'that', 'the', 'shared', 'feature', 'space', 'could', 'contain', 'some', 'unnecessary', 'task-specific', 'features', ',', 'while', 'some', 'sharable', 'features', 'could', 'also', 'be', 'mixed', 'in', 'private', 'space', ',', 'suffering', 'from', 'feature', 'redundancy', '.', 'Taking', 'the', 'following', 'two', 'sentences', 'as', 'examples', ',', 'which', 'are', 'extracted', 'from', 'two', 'different', 'sentiment', 'classification', 'tasks', ':', 'Movie', 'reviews', 'and', 'Baby', 'products', 'reviews', '.', 'The', 'infantile', 'cart', 'is', 'simple', 'and', 'easy', 'to', 'use', '.', 'This', 'kind', 'of', 'humour', 'is', 'infantile', 'and', 'boring', '.', 'The', 'word', '�infantile�', 'indicates', 'negative', 'sentiment', 'in', 'Movie', 'task', 'while', 'it', 'is', 'neutral', 'in', 'Baby', 'task', '.', 'However', ',', 'the', 'general', 'shared-private', 'model', 'could', 'place', 'the', 'task-specific', 'word', '�infantile�', 'in', 'a', 'shared', 'space', ',', 'leaving', 'potential', 'hazards', 'for', 'other', 'tasks', '.', 'Additionally', ',', 'the', 'capacity', 'of', 'shared', 'space', 'could', 'also', 'be', 'wasted', 'by', 'some', 'unnecessary', 'features', '.', 'To', 'address', 'this', 'problem', ',', 'in', 'this', 'paper', 'we', 'propose', 'an', 'adversarial', 'multi-task', 'framework', ',', 'in', 'which', 'the', 'shared', 'and', 'private', 'feature', 'spaces', 'are', 'in', 'herently', 'disjoint', 'by', 'introducing', 'orthogonality', 'constraints.Specifically', ',', 'we', 'design', 'a', 'generic', 'shared', 'private', 'learning', 'framework', 'to', 'model', 'the', 'text', 'sequence', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oikCyx83xD6A"
      },
      "source": [
        "**Tokenization using the spaCy library**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZU94bbURxNtK",
        "outputId": "24588e75-7e5c-4139-cbbd-befcc6ca3c8c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from spacy.lang.en import English\n",
        "\n",
        "# Load English tokenizer, tagger, parser, NER and word vectors\n",
        "nlp = English()\n",
        "\n",
        "#  \"nlp\" Object is used to create documents with linguistic annotations.\n",
        "my_doc = nlp(text)\n",
        "\n",
        "# Create list of word tokens\n",
        "token_list = []\n",
        "for token in my_doc:\n",
        "    token_list.append(token.text)\n",
        "print(list(iter(token_list)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Neural', 'network', 'models', 'have', 'shown', 'their', 'promising', 'opportunities', 'for', 'multi', '-', 'task', '\\n', 'learning', ',', 'which', 'focus', 'on', 'learning', 'the', 'shared', 'layers', 'to', 'extract', 'the', 'common', 'and', '\\n', 'task', '-', 'invariant', 'features', '.', 'However', ',', 'in', 'most', 'existing', 'approaches', ',', 'the', 'extracted', 'shared', '\\n', 'features', 'are', 'prone', 'to', 'be', 'contaminated', 'by', 'task', '-', 'specific', 'features', 'or', 'the', 'noise', 'brought', '\\n', 'by', 'other', 'tasks', '.', 'In', 'this', 'paper', ',', 'we', 'propose', 'an', 'adversarial', 'multi', '-', 'task', 'learning', 'framework', ',', '\\n', 'alleviating', 'the', 'shared', 'and', 'private', 'latent', 'feature', 'spaces', 'from', 'interfering', 'with', '\\n', 'each', 'other', '.', 'We', 'conduct', 'extensive', 'experiments', 'on', '16', 'different', 'text', 'classification', '\\n', 'tasks', ',', 'which', 'demonstrates', 'the', 'benefits', 'of', 'our', 'approach', '.', 'Besides', ',', 'we', 'show', 'that', 'the', '\\n', 'shared', 'knowledge', 'learned', 'by', 'our', 'proposed', 'model', 'can', 'be', 'regarded', 'as', 'off', '-', 'the', '-', 'shelf', '\\n', 'knowledge', 'and', 'easily', 'transferred', 'to', 'new', 'tasks', '.', '\\n\\n', 'Multi', '-', 'task', 'learning', 'is', 'an', 'effective', 'approach', 'to', 'improve', 'the', 'performance', 'of', 'a', 'single', 'task', 'with', '\\n', 'the', 'help', 'of', 'other', 'related', 'tasks', '.', 'Recently', ',', 'neural', '-', 'based', 'models', 'for', 'multi', '-', 'task', 'learning', 'have', 'become', '\\n', 'very', 'popular', ',', 'ranging', 'from', 'computer', 'vision', '(', 'Misra', 'et', 'al', '.', ',', '2016', ';', 'Zhang', 'et', 'al', '.', ',', '2014', ')', 'to', 'natural', '\\n', 'language', 'processing', '(', 'Collobert', 'andWeston', ',', '2008', ';', 'Luong', 'et', 'al', '.', ',', '2015', ')', ',', 'since', 'they', 'provide', 'a', 'convenient', '\\n', 'way', 'of', 'combining', 'information', 'from', 'multiple', 'tasks', '.', '\\n', 'However', ',', 'most', 'existing', 'work', 'on', 'multi', '-', 'task', 'learning', '(', 'Liu', 'et', 'al', '.', ',', '2016c', ',', 'b', ')', 'attempts', 'to', 'divide', 'the', '\\n', 'features', 'of', 'different', 'tasks', 'into', 'private', 'and', 'shared', 'spaces', ',', 'merely', 'based', 'on', 'whether', 'parameters', 'of', 'some', 'components', 'should', 'be', 'shared', '.', 'As', 'shown', 'in', 'Figure', '1-(a', ')', ',', 'the', 'general', 'shared', '-', 'private', 'model', 'introduces', '\\n', 'two', 'feature', 'spaces', 'for', 'any', 'task', ':', 'one', 'is', 'used', 'to', 'store', 'task', '-', 'dependent', 'features', ',', 'the', 'other', 'is', '\\n', 'used', 'to', 'capture', 'shared', 'features', '.', 'The', 'major', 'limitation', 'of', 'this', 'framework', 'is', 'that', 'the', 'shared', 'feature', '\\n', 'space', 'could', 'contain', 'some', 'unnecessary', 'task', '-', 'specific', 'features', ',', 'while', 'some', 'sharable', 'features', '\\n', 'could', 'also', 'be', 'mixed', 'in', 'private', 'space', ',', 'suffering', 'from', 'feature', 'redundancy', '.', '\\n', 'Taking', 'the', 'following', 'two', 'sentences', 'as', 'examples', ',', 'which', 'are', 'extracted', 'from', 'two', 'different', 'sentiment', '\\n', 'classification', 'tasks', ':', 'Movie', 'reviews', 'and', 'Baby', 'products', 'reviews', '.', '\\n', 'The', 'infantile', 'cart', 'is', 'simple', 'and', 'easy', 'to', 'use', '.', 'This', 'kind', 'of', 'humour', 'is', 'infantile', 'and', 'boring', '.', '\\n', 'The', 'word', '�', 'infantile', '�', 'indicates', 'negative', 'sentiment', 'in', 'Movie', 'task', 'while', 'it', 'is', 'neutral', 'in', 'Baby', 'task', '.', '\\n', 'However', ',', 'the', 'general', 'shared', '-', 'private', 'model', 'could', 'place', 'the', 'task', '-', 'specific', 'word', '�', 'infantile', '�', 'in', 'a', '\\n', 'shared', 'space', ',', 'leaving', 'potential', 'hazards', 'for', 'other', 'tasks', '.', 'Additionally', ',', 'the', 'capacity', 'of', 'shared', 'space', '\\n', 'could', 'also', 'be', 'wasted', 'by', 'some', 'unnecessary', 'features', '.', '\\n', 'To', 'address', 'this', 'problem', ',', 'in', 'this', 'paper', 'we', 'propose', 'an', 'adversarial', 'multi', '-', 'task', 'framework', ',', 'in', '\\n', 'which', 'the', 'shared', 'and', 'private', 'feature', 'spaces', 'are', 'in', 'herently', 'disjoint', 'by', 'introducing', 'orthogonality', 'constraints', '.', 'Specifically', ',', 'we', 'design', 'a', 'generic', 'shared', 'private', 'learning', 'framework', 'to', 'model', 'the', 'text', 'sequence', '.', '\\n\\n']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZykVhHYyq36"
      },
      "source": [
        "**Tokenization using Keras**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAzl4puFyhPx",
        "outputId": "6006c3fe-0e7f-4b6a-8495-d3d41ded6e79",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from keras.preprocessing.text import text_to_word_sequence\n",
        "# tokenize\n",
        "result = text_to_word_sequence(text)\n",
        "print(list(iter(result)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['neural', 'network', 'models', 'have', 'shown', 'their', 'promising', 'opportunities', 'for', 'multi', 'task', 'learning', 'which', 'focus', 'on', 'learning', 'the', 'shared', 'layers', 'to', 'extract', 'the', 'common', 'and', 'task', 'invariant', 'features', 'however', 'in', 'most', 'existing', 'approaches', 'the', 'extracted', 'shared', 'features', 'are', 'prone', 'to', 'be', 'contaminated', 'by', 'task', 'specific', 'features', 'or', 'the', 'noise', 'brought', 'by', 'other', 'tasks', 'in', 'this', 'paper', 'we', 'propose', 'an', 'adversarial', 'multi', 'task', 'learning', 'framework', 'alleviating', 'the', 'shared', 'and', 'private', 'latent', 'feature', 'spaces', 'from', 'interfering', 'with', 'each', 'other', 'we', 'conduct', 'extensive', 'experiments', 'on', '16', 'different', 'text', 'classification', 'tasks', 'which', 'demonstrates', 'the', 'benefits', 'of', 'our', 'approach', 'besides', 'we', 'show', 'that', 'the', 'shared', 'knowledge', 'learned', 'by', 'our', 'proposed', 'model', 'can', 'be', 'regarded', 'as', 'off', 'the', 'shelf', 'knowledge', 'and', 'easily', 'transferred', 'to', 'new', 'tasks', 'multi', 'task', 'learning', 'is', 'an', 'effective', 'approach', 'to', 'improve', 'the', 'performance', 'of', 'a', 'single', 'task', 'with', 'the', 'help', 'of', 'other', 'related', 'tasks', 'recently', 'neural', 'based', 'models', 'for', 'multi', 'task', 'learning', 'have', 'become', 'very', 'popular', 'ranging', 'from', 'computer', 'vision', 'misra', 'et', 'al', '2016', 'zhang', 'et', 'al', '2014', 'to', 'natural', 'language', 'processing', 'collobert', 'andweston', '2008', 'luong', 'et', 'al', '2015', 'since', 'they', 'provide', 'a', 'convenient', 'way', 'of', 'combining', 'information', 'from', 'multiple', 'tasks', 'however', 'most', 'existing', 'work', 'on', 'multi', 'task', 'learning', 'liu', 'et', 'al', '2016c', 'b', 'attempts', 'to', 'divide', 'the', 'features', 'of', 'different', 'tasks', 'into', 'private', 'and', 'shared', 'spaces', 'merely', 'based', 'on', 'whether', 'parameters', 'of', 'some', 'components', 'should', 'be', 'shared', 'as', 'shown', 'in', 'figure', '1', 'a', 'the', 'general', 'shared', 'private', 'model', 'introduces', 'two', 'feature', 'spaces', 'for', 'any', 'task', 'one', 'is', 'used', 'to', 'store', 'task', 'dependent', 'features', 'the', 'other', 'is', 'used', 'to', 'capture', 'shared', 'features', 'the', 'major', 'limitation', 'of', 'this', 'framework', 'is', 'that', 'the', 'shared', 'feature', 'space', 'could', 'contain', 'some', 'unnecessary', 'task', 'specific', 'features', 'while', 'some', 'sharable', 'features', 'could', 'also', 'be', 'mixed', 'in', 'private', 'space', 'suffering', 'from', 'feature', 'redundancy', 'taking', 'the', 'following', 'two', 'sentences', 'as', 'examples', 'which', 'are', 'extracted', 'from', 'two', 'different', 'sentiment', 'classification', 'tasks', 'movie', 'reviews', 'and', 'baby', 'products', 'reviews', 'the', 'infantile', 'cart', 'is', 'simple', 'and', 'easy', 'to', 'use', 'this', 'kind', 'of', 'humour', 'is', 'infantile', 'and', 'boring', 'the', 'word', '�infantile�', 'indicates', 'negative', 'sentiment', 'in', 'movie', 'task', 'while', 'it', 'is', 'neutral', 'in', 'baby', 'task', 'however', 'the', 'general', 'shared', 'private', 'model', 'could', 'place', 'the', 'task', 'specific', 'word', '�infantile�', 'in', 'a', 'shared', 'space', 'leaving', 'potential', 'hazards', 'for', 'other', 'tasks', 'additionally', 'the', 'capacity', 'of', 'shared', 'space', 'could', 'also', 'be', 'wasted', 'by', 'some', 'unnecessary', 'features', 'to', 'address', 'this', 'problem', 'in', 'this', 'paper', 'we', 'propose', 'an', 'adversarial', 'multi', 'task', 'framework', 'in', 'which', 'the', 'shared', 'and', 'private', 'feature', 'spaces', 'are', 'in', 'herently', 'disjoint', 'by', 'introducing', 'orthogonality', 'constraints', 'specifically', 'we', 'design', 'a', 'generic', 'shared', 'private', 'learning', 'framework', 'to', 'model', 'the', 'text', 'sequence']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNZL96CCzcvv"
      },
      "source": [
        "**Tokenization using Gensim**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RPYr_n_yr8n",
        "outputId": "8e8602ff-5fe4-4058-95e4-a14103a19ac1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from gensim.utils import tokenize\n",
        "tokenize_text_gensim = tokenize(text)\n",
        "tokens = tokenize(text)\n",
        "print(list(iter(list(tokenize(text)))))"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Neural', 'network', 'models', 'have', 'shown', 'their', 'promising', 'opportunities', 'for', 'multi', 'task', 'learning', 'which', 'focus', 'on', 'learning', 'the', 'shared', 'layers', 'to', 'extract', 'the', 'common', 'and', 'task', 'invariant', 'features', 'However', 'in', 'most', 'existing', 'approaches', 'the', 'extracted', 'shared', 'features', 'are', 'prone', 'to', 'be', 'contaminated', 'by', 'task', 'specific', 'features', 'or', 'the', 'noise', 'brought', 'by', 'other', 'tasks', 'In', 'this', 'paper', 'we', 'propose', 'an', 'adversarial', 'multi', 'task', 'learning', 'framework', 'alleviating', 'the', 'shared', 'and', 'private', 'latent', 'feature', 'spaces', 'from', 'interfering', 'with', 'each', 'other', 'We', 'conduct', 'extensive', 'experiments', 'on', 'different', 'text', 'classification', 'tasks', 'which', 'demonstrates', 'the', 'benefits', 'of', 'our', 'approach', 'Besides', 'we', 'show', 'that', 'the', 'shared', 'knowledge', 'learned', 'by', 'our', 'proposed', 'model', 'can', 'be', 'regarded', 'as', 'off', 'the', 'shelf', 'knowledge', 'and', 'easily', 'transferred', 'to', 'new', 'tasks', 'Multi', 'task', 'learning', 'is', 'an', 'effective', 'approach', 'to', 'improve', 'the', 'performance', 'of', 'a', 'single', 'task', 'with', 'the', 'help', 'of', 'other', 'related', 'tasks', 'Recently', 'neural', 'based', 'models', 'for', 'multi', 'task', 'learning', 'have', 'become', 'very', 'popular', 'ranging', 'from', 'computer', 'vision', 'Misra', 'et', 'al', 'Zhang', 'et', 'al', 'to', 'natural', 'language', 'processing', 'Collobert', 'andWeston', 'Luong', 'et', 'al', 'since', 'they', 'provide', 'a', 'convenient', 'way', 'of', 'combining', 'information', 'from', 'multiple', 'tasks', 'However', 'most', 'existing', 'work', 'on', 'multi', 'task', 'learning', 'Liu', 'et', 'al', 'c', 'b', 'attempts', 'to', 'divide', 'the', 'features', 'of', 'different', 'tasks', 'into', 'private', 'and', 'shared', 'spaces', 'merely', 'based', 'on', 'whether', 'parameters', 'of', 'some', 'components', 'should', 'be', 'shared', 'As', 'shown', 'in', 'Figure', 'a', 'the', 'general', 'shared', 'private', 'model', 'introduces', 'two', 'feature', 'spaces', 'for', 'any', 'task', 'one', 'is', 'used', 'to', 'store', 'task', 'dependent', 'features', 'the', 'other', 'is', 'used', 'to', 'capture', 'shared', 'features', 'The', 'major', 'limitation', 'of', 'this', 'framework', 'is', 'that', 'the', 'shared', 'feature', 'space', 'could', 'contain', 'some', 'unnecessary', 'task', 'specific', 'features', 'while', 'some', 'sharable', 'features', 'could', 'also', 'be', 'mixed', 'in', 'private', 'space', 'suffering', 'from', 'feature', 'redundancy', 'Taking', 'the', 'following', 'two', 'sentences', 'as', 'examples', 'which', 'are', 'extracted', 'from', 'two', 'different', 'sentiment', 'classification', 'tasks', 'Movie', 'reviews', 'and', 'Baby', 'products', 'reviews', 'The', 'infantile', 'cart', 'is', 'simple', 'and', 'easy', 'to', 'use', 'This', 'kind', 'of', 'humour', 'is', 'infantile', 'and', 'boring', 'The', 'word', 'infantile', 'indicates', 'negative', 'sentiment', 'in', 'Movie', 'task', 'while', 'it', 'is', 'neutral', 'in', 'Baby', 'task', 'However', 'the', 'general', 'shared', 'private', 'model', 'could', 'place', 'the', 'task', 'specific', 'word', 'infantile', 'in', 'a', 'shared', 'space', 'leaving', 'potential', 'hazards', 'for', 'other', 'tasks', 'Additionally', 'the', 'capacity', 'of', 'shared', 'space', 'could', 'also', 'be', 'wasted', 'by', 'some', 'unnecessary', 'features', 'To', 'address', 'this', 'problem', 'in', 'this', 'paper', 'we', 'propose', 'an', 'adversarial', 'multi', 'task', 'framework', 'in', 'which', 'the', 'shared', 'and', 'private', 'feature', 'spaces', 'are', 'in', 'herently', 'disjoint', 'by', 'introducing', 'orthogonality', 'constraints', 'Specifically', 'we', 'design', 'a', 'generic', 'shared', 'private', 'learning', 'framework', 'to', 'model', 'the', 'text', 'sequence']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9e0teKDlcMEU"
      },
      "source": [
        "from gensim.utils import tokenize\n",
        "tokenize_text_gensim = tokenize(text)\n",
        "tokens = list(tokenize(text))\n",
        "with open('/content/reserch_paper_out_tokenize.txt', 'w') as writefile:\n",
        "    writefile.write(str(tokens))\n"
      ],
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mG03zF4mUb6"
      },
      "source": [
        "# stemmer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DwrRjfl_7FUl"
      },
      "source": [
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vda1kmxl6jQZ",
        "outputId": "3c8638e0-c951-40e2-c486-31955850b694",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from nltk.stem.porter import *\n",
        "stemmer = PorterStemmer()\n",
        "for token in tokens:\n",
        "    print(token + ' --> ' + stemmer.stem(token))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "network --> network\n",
            "models --> model\n",
            "have --> have\n",
            "shown --> shown\n",
            "their --> their\n",
            "promising --> promis\n",
            "opportunities --> opportun\n",
            "for --> for\n",
            "multi --> multi\n",
            "task --> task\n",
            "learning --> learn\n",
            "which --> which\n",
            "focus --> focu\n",
            "on --> on\n",
            "learning --> learn\n",
            "the --> the\n",
            "shared --> share\n",
            "layers --> layer\n",
            "to --> to\n",
            "extract --> extract\n",
            "the --> the\n",
            "common --> common\n",
            "and --> and\n",
            "task --> task\n",
            "invariant --> invari\n",
            "features --> featur\n",
            "However --> howev\n",
            "in --> in\n",
            "most --> most\n",
            "existing --> exist\n",
            "approaches --> approach\n",
            "the --> the\n",
            "extracted --> extract\n",
            "shared --> share\n",
            "features --> featur\n",
            "are --> are\n",
            "prone --> prone\n",
            "to --> to\n",
            "be --> be\n",
            "contaminated --> contamin\n",
            "by --> by\n",
            "task --> task\n",
            "specific --> specif\n",
            "features --> featur\n",
            "or --> or\n",
            "the --> the\n",
            "noise --> nois\n",
            "brought --> brought\n",
            "by --> by\n",
            "other --> other\n",
            "tasks --> task\n",
            "In --> In\n",
            "this --> thi\n",
            "paper --> paper\n",
            "we --> we\n",
            "propose --> propos\n",
            "an --> an\n",
            "adversarial --> adversari\n",
            "multi --> multi\n",
            "task --> task\n",
            "learning --> learn\n",
            "framework --> framework\n",
            "alleviating --> allevi\n",
            "the --> the\n",
            "shared --> share\n",
            "and --> and\n",
            "private --> privat\n",
            "latent --> latent\n",
            "feature --> featur\n",
            "spaces --> space\n",
            "from --> from\n",
            "interfering --> interf\n",
            "with --> with\n",
            "each --> each\n",
            "other --> other\n",
            "We --> We\n",
            "conduct --> conduct\n",
            "extensive --> extens\n",
            "experiments --> experi\n",
            "on --> on\n",
            "different --> differ\n",
            "text --> text\n",
            "classification --> classif\n",
            "tasks --> task\n",
            "which --> which\n",
            "demonstrates --> demonstr\n",
            "the --> the\n",
            "benefits --> benefit\n",
            "of --> of\n",
            "our --> our\n",
            "approach --> approach\n",
            "Besides --> besid\n",
            "we --> we\n",
            "show --> show\n",
            "that --> that\n",
            "the --> the\n",
            "shared --> share\n",
            "knowledge --> knowledg\n",
            "learned --> learn\n",
            "by --> by\n",
            "our --> our\n",
            "proposed --> propos\n",
            "model --> model\n",
            "can --> can\n",
            "be --> be\n",
            "regarded --> regard\n",
            "as --> as\n",
            "off --> off\n",
            "the --> the\n",
            "shelf --> shelf\n",
            "knowledge --> knowledg\n",
            "and --> and\n",
            "easily --> easili\n",
            "transferred --> transfer\n",
            "to --> to\n",
            "new --> new\n",
            "tasks --> task\n",
            "Multi --> multi\n",
            "task --> task\n",
            "learning --> learn\n",
            "is --> is\n",
            "an --> an\n",
            "effective --> effect\n",
            "approach --> approach\n",
            "to --> to\n",
            "improve --> improv\n",
            "the --> the\n",
            "performance --> perform\n",
            "of --> of\n",
            "a --> a\n",
            "single --> singl\n",
            "task --> task\n",
            "with --> with\n",
            "the --> the\n",
            "help --> help\n",
            "of --> of\n",
            "other --> other\n",
            "related --> relat\n",
            "tasks --> task\n",
            "Recently --> recent\n",
            "neural --> neural\n",
            "based --> base\n",
            "models --> model\n",
            "for --> for\n",
            "multi --> multi\n",
            "task --> task\n",
            "learning --> learn\n",
            "have --> have\n",
            "become --> becom\n",
            "very --> veri\n",
            "popular --> popular\n",
            "ranging --> rang\n",
            "from --> from\n",
            "computer --> comput\n",
            "vision --> vision\n",
            "Misra --> misra\n",
            "et --> et\n",
            "al --> al\n",
            "Zhang --> zhang\n",
            "et --> et\n",
            "al --> al\n",
            "to --> to\n",
            "natural --> natur\n",
            "language --> languag\n",
            "processing --> process\n",
            "Collobert --> collobert\n",
            "andWeston --> andweston\n",
            "Luong --> luong\n",
            "et --> et\n",
            "al --> al\n",
            "since --> sinc\n",
            "they --> they\n",
            "provide --> provid\n",
            "a --> a\n",
            "convenient --> conveni\n",
            "way --> way\n",
            "of --> of\n",
            "combining --> combin\n",
            "information --> inform\n",
            "from --> from\n",
            "multiple --> multipl\n",
            "tasks --> task\n",
            "However --> howev\n",
            "most --> most\n",
            "existing --> exist\n",
            "work --> work\n",
            "on --> on\n",
            "multi --> multi\n",
            "task --> task\n",
            "learning --> learn\n",
            "Liu --> liu\n",
            "et --> et\n",
            "al --> al\n",
            "c --> c\n",
            "b --> b\n",
            "attempts --> attempt\n",
            "to --> to\n",
            "divide --> divid\n",
            "the --> the\n",
            "features --> featur\n",
            "of --> of\n",
            "different --> differ\n",
            "tasks --> task\n",
            "into --> into\n",
            "private --> privat\n",
            "and --> and\n",
            "shared --> share\n",
            "spaces --> space\n",
            "merely --> mere\n",
            "based --> base\n",
            "on --> on\n",
            "whether --> whether\n",
            "parameters --> paramet\n",
            "of --> of\n",
            "some --> some\n",
            "components --> compon\n",
            "should --> should\n",
            "be --> be\n",
            "shared --> share\n",
            "As --> As\n",
            "shown --> shown\n",
            "in --> in\n",
            "Figure --> figur\n",
            "a --> a\n",
            "the --> the\n",
            "general --> gener\n",
            "shared --> share\n",
            "private --> privat\n",
            "model --> model\n",
            "introduces --> introduc\n",
            "two --> two\n",
            "feature --> featur\n",
            "spaces --> space\n",
            "for --> for\n",
            "any --> ani\n",
            "task --> task\n",
            "one --> one\n",
            "is --> is\n",
            "used --> use\n",
            "to --> to\n",
            "store --> store\n",
            "task --> task\n",
            "dependent --> depend\n",
            "features --> featur\n",
            "the --> the\n",
            "other --> other\n",
            "is --> is\n",
            "used --> use\n",
            "to --> to\n",
            "capture --> captur\n",
            "shared --> share\n",
            "features --> featur\n",
            "The --> the\n",
            "major --> major\n",
            "limitation --> limit\n",
            "of --> of\n",
            "this --> thi\n",
            "framework --> framework\n",
            "is --> is\n",
            "that --> that\n",
            "the --> the\n",
            "shared --> share\n",
            "feature --> featur\n",
            "space --> space\n",
            "could --> could\n",
            "contain --> contain\n",
            "some --> some\n",
            "unnecessary --> unnecessari\n",
            "task --> task\n",
            "specific --> specif\n",
            "features --> featur\n",
            "while --> while\n",
            "some --> some\n",
            "sharable --> sharabl\n",
            "features --> featur\n",
            "could --> could\n",
            "also --> also\n",
            "be --> be\n",
            "mixed --> mix\n",
            "in --> in\n",
            "private --> privat\n",
            "space --> space\n",
            "suffering --> suffer\n",
            "from --> from\n",
            "feature --> featur\n",
            "redundancy --> redund\n",
            "Taking --> take\n",
            "the --> the\n",
            "following --> follow\n",
            "two --> two\n",
            "sentences --> sentenc\n",
            "as --> as\n",
            "examples --> exampl\n",
            "which --> which\n",
            "are --> are\n",
            "extracted --> extract\n",
            "from --> from\n",
            "two --> two\n",
            "different --> differ\n",
            "sentiment --> sentiment\n",
            "classification --> classif\n",
            "tasks --> task\n",
            "Movie --> movi\n",
            "reviews --> review\n",
            "and --> and\n",
            "Baby --> babi\n",
            "products --> product\n",
            "reviews --> review\n",
            "The --> the\n",
            "infantile --> infantil\n",
            "cart --> cart\n",
            "is --> is\n",
            "simple --> simpl\n",
            "and --> and\n",
            "easy --> easi\n",
            "to --> to\n",
            "use --> use\n",
            "This --> thi\n",
            "kind --> kind\n",
            "of --> of\n",
            "humour --> humour\n",
            "is --> is\n",
            "infantile --> infantil\n",
            "and --> and\n",
            "boring --> bore\n",
            "The --> the\n",
            "word --> word\n",
            "infantile --> infantil\n",
            "indicates --> indic\n",
            "negative --> neg\n",
            "sentiment --> sentiment\n",
            "in --> in\n",
            "Movie --> movi\n",
            "task --> task\n",
            "while --> while\n",
            "it --> it\n",
            "is --> is\n",
            "neutral --> neutral\n",
            "in --> in\n",
            "Baby --> babi\n",
            "task --> task\n",
            "However --> howev\n",
            "the --> the\n",
            "general --> gener\n",
            "shared --> share\n",
            "private --> privat\n",
            "model --> model\n",
            "could --> could\n",
            "place --> place\n",
            "the --> the\n",
            "task --> task\n",
            "specific --> specif\n",
            "word --> word\n",
            "infantile --> infantil\n",
            "in --> in\n",
            "a --> a\n",
            "shared --> share\n",
            "space --> space\n",
            "leaving --> leav\n",
            "potential --> potenti\n",
            "hazards --> hazard\n",
            "for --> for\n",
            "other --> other\n",
            "tasks --> task\n",
            "Additionally --> addit\n",
            "the --> the\n",
            "capacity --> capac\n",
            "of --> of\n",
            "shared --> share\n",
            "space --> space\n",
            "could --> could\n",
            "also --> also\n",
            "be --> be\n",
            "wasted --> wast\n",
            "by --> by\n",
            "some --> some\n",
            "unnecessary --> unnecessari\n",
            "features --> featur\n",
            "To --> To\n",
            "address --> address\n",
            "this --> thi\n",
            "problem --> problem\n",
            "in --> in\n",
            "this --> thi\n",
            "paper --> paper\n",
            "we --> we\n",
            "propose --> propos\n",
            "an --> an\n",
            "adversarial --> adversari\n",
            "multi --> multi\n",
            "task --> task\n",
            "framework --> framework\n",
            "in --> in\n",
            "which --> which\n",
            "the --> the\n",
            "shared --> share\n",
            "and --> and\n",
            "private --> privat\n",
            "feature --> featur\n",
            "spaces --> space\n",
            "are --> are\n",
            "in --> in\n",
            "herently --> herent\n",
            "disjoint --> disjoint\n",
            "by --> by\n",
            "introducing --> introduc\n",
            "orthogonality --> orthogon\n",
            "constraints --> constraint\n",
            "Specifically --> specif\n",
            "we --> we\n",
            "design --> design\n",
            "a --> a\n",
            "generic --> gener\n",
            "shared --> share\n",
            "private --> privat\n",
            "learning --> learn\n",
            "framework --> framework\n",
            "to --> to\n",
            "model --> model\n",
            "the --> the\n",
            "text --> text\n",
            "sequence --> sequenc\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRdxJxak-Hns",
        "outputId": "ce630228-e988-4a92-af6d-d849b6ba3b39",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from nltk.stem.snowball import SnowballStemmer\n",
        "\n",
        "stemmer = SnowballStemmer(language='english')\n",
        "\n",
        "for token in tokens:\n",
        "    print(token + ' --> ' + stemmer.stem(token))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Neural --> neural\n",
            "network --> network\n",
            "models --> model\n",
            "have --> have\n",
            "shown --> shown\n",
            "their --> their\n",
            "promising --> promis\n",
            "opportunities --> opportun\n",
            "for --> for\n",
            "multi --> multi\n",
            "task --> task\n",
            "learning --> learn\n",
            "which --> which\n",
            "focus --> focus\n",
            "on --> on\n",
            "learning --> learn\n",
            "the --> the\n",
            "shared --> share\n",
            "layers --> layer\n",
            "to --> to\n",
            "extract --> extract\n",
            "the --> the\n",
            "common --> common\n",
            "and --> and\n",
            "task --> task\n",
            "invariant --> invari\n",
            "features --> featur\n",
            "However --> howev\n",
            "in --> in\n",
            "most --> most\n",
            "existing --> exist\n",
            "approaches --> approach\n",
            "the --> the\n",
            "extracted --> extract\n",
            "shared --> share\n",
            "features --> featur\n",
            "are --> are\n",
            "prone --> prone\n",
            "to --> to\n",
            "be --> be\n",
            "contaminated --> contamin\n",
            "by --> by\n",
            "task --> task\n",
            "specific --> specif\n",
            "features --> featur\n",
            "or --> or\n",
            "the --> the\n",
            "noise --> nois\n",
            "brought --> brought\n",
            "by --> by\n",
            "other --> other\n",
            "tasks --> task\n",
            "In --> in\n",
            "this --> this\n",
            "paper --> paper\n",
            "we --> we\n",
            "propose --> propos\n",
            "an --> an\n",
            "adversarial --> adversari\n",
            "multi --> multi\n",
            "task --> task\n",
            "learning --> learn\n",
            "framework --> framework\n",
            "alleviating --> allevi\n",
            "the --> the\n",
            "shared --> share\n",
            "and --> and\n",
            "private --> privat\n",
            "latent --> latent\n",
            "feature --> featur\n",
            "spaces --> space\n",
            "from --> from\n",
            "interfering --> interf\n",
            "with --> with\n",
            "each --> each\n",
            "other --> other\n",
            "We --> we\n",
            "conduct --> conduct\n",
            "extensive --> extens\n",
            "experiments --> experi\n",
            "on --> on\n",
            "different --> differ\n",
            "text --> text\n",
            "classification --> classif\n",
            "tasks --> task\n",
            "which --> which\n",
            "demonstrates --> demonstr\n",
            "the --> the\n",
            "benefits --> benefit\n",
            "of --> of\n",
            "our --> our\n",
            "approach --> approach\n",
            "Besides --> besid\n",
            "we --> we\n",
            "show --> show\n",
            "that --> that\n",
            "the --> the\n",
            "shared --> share\n",
            "knowledge --> knowledg\n",
            "learned --> learn\n",
            "by --> by\n",
            "our --> our\n",
            "proposed --> propos\n",
            "model --> model\n",
            "can --> can\n",
            "be --> be\n",
            "regarded --> regard\n",
            "as --> as\n",
            "off --> off\n",
            "the --> the\n",
            "shelf --> shelf\n",
            "knowledge --> knowledg\n",
            "and --> and\n",
            "easily --> easili\n",
            "transferred --> transfer\n",
            "to --> to\n",
            "new --> new\n",
            "tasks --> task\n",
            "Multi --> multi\n",
            "task --> task\n",
            "learning --> learn\n",
            "is --> is\n",
            "an --> an\n",
            "effective --> effect\n",
            "approach --> approach\n",
            "to --> to\n",
            "improve --> improv\n",
            "the --> the\n",
            "performance --> perform\n",
            "of --> of\n",
            "a --> a\n",
            "single --> singl\n",
            "task --> task\n",
            "with --> with\n",
            "the --> the\n",
            "help --> help\n",
            "of --> of\n",
            "other --> other\n",
            "related --> relat\n",
            "tasks --> task\n",
            "Recently --> recent\n",
            "neural --> neural\n",
            "based --> base\n",
            "models --> model\n",
            "for --> for\n",
            "multi --> multi\n",
            "task --> task\n",
            "learning --> learn\n",
            "have --> have\n",
            "become --> becom\n",
            "very --> veri\n",
            "popular --> popular\n",
            "ranging --> rang\n",
            "from --> from\n",
            "computer --> comput\n",
            "vision --> vision\n",
            "Misra --> misra\n",
            "et --> et\n",
            "al --> al\n",
            "Zhang --> zhang\n",
            "et --> et\n",
            "al --> al\n",
            "to --> to\n",
            "natural --> natur\n",
            "language --> languag\n",
            "processing --> process\n",
            "Collobert --> collobert\n",
            "andWeston --> andweston\n",
            "Luong --> luong\n",
            "et --> et\n",
            "al --> al\n",
            "since --> sinc\n",
            "they --> they\n",
            "provide --> provid\n",
            "a --> a\n",
            "convenient --> conveni\n",
            "way --> way\n",
            "of --> of\n",
            "combining --> combin\n",
            "information --> inform\n",
            "from --> from\n",
            "multiple --> multipl\n",
            "tasks --> task\n",
            "However --> howev\n",
            "most --> most\n",
            "existing --> exist\n",
            "work --> work\n",
            "on --> on\n",
            "multi --> multi\n",
            "task --> task\n",
            "learning --> learn\n",
            "Liu --> liu\n",
            "et --> et\n",
            "al --> al\n",
            "c --> c\n",
            "b --> b\n",
            "attempts --> attempt\n",
            "to --> to\n",
            "divide --> divid\n",
            "the --> the\n",
            "features --> featur\n",
            "of --> of\n",
            "different --> differ\n",
            "tasks --> task\n",
            "into --> into\n",
            "private --> privat\n",
            "and --> and\n",
            "shared --> share\n",
            "spaces --> space\n",
            "merely --> mere\n",
            "based --> base\n",
            "on --> on\n",
            "whether --> whether\n",
            "parameters --> paramet\n",
            "of --> of\n",
            "some --> some\n",
            "components --> compon\n",
            "should --> should\n",
            "be --> be\n",
            "shared --> share\n",
            "As --> as\n",
            "shown --> shown\n",
            "in --> in\n",
            "Figure --> figur\n",
            "a --> a\n",
            "the --> the\n",
            "general --> general\n",
            "shared --> share\n",
            "private --> privat\n",
            "model --> model\n",
            "introduces --> introduc\n",
            "two --> two\n",
            "feature --> featur\n",
            "spaces --> space\n",
            "for --> for\n",
            "any --> ani\n",
            "task --> task\n",
            "one --> one\n",
            "is --> is\n",
            "used --> use\n",
            "to --> to\n",
            "store --> store\n",
            "task --> task\n",
            "dependent --> depend\n",
            "features --> featur\n",
            "the --> the\n",
            "other --> other\n",
            "is --> is\n",
            "used --> use\n",
            "to --> to\n",
            "capture --> captur\n",
            "shared --> share\n",
            "features --> featur\n",
            "The --> the\n",
            "major --> major\n",
            "limitation --> limit\n",
            "of --> of\n",
            "this --> this\n",
            "framework --> framework\n",
            "is --> is\n",
            "that --> that\n",
            "the --> the\n",
            "shared --> share\n",
            "feature --> featur\n",
            "space --> space\n",
            "could --> could\n",
            "contain --> contain\n",
            "some --> some\n",
            "unnecessary --> unnecessari\n",
            "task --> task\n",
            "specific --> specif\n",
            "features --> featur\n",
            "while --> while\n",
            "some --> some\n",
            "sharable --> sharabl\n",
            "features --> featur\n",
            "could --> could\n",
            "also --> also\n",
            "be --> be\n",
            "mixed --> mix\n",
            "in --> in\n",
            "private --> privat\n",
            "space --> space\n",
            "suffering --> suffer\n",
            "from --> from\n",
            "feature --> featur\n",
            "redundancy --> redund\n",
            "Taking --> take\n",
            "the --> the\n",
            "following --> follow\n",
            "two --> two\n",
            "sentences --> sentenc\n",
            "as --> as\n",
            "examples --> exampl\n",
            "which --> which\n",
            "are --> are\n",
            "extracted --> extract\n",
            "from --> from\n",
            "two --> two\n",
            "different --> differ\n",
            "sentiment --> sentiment\n",
            "classification --> classif\n",
            "tasks --> task\n",
            "Movie --> movi\n",
            "reviews --> review\n",
            "and --> and\n",
            "Baby --> babi\n",
            "products --> product\n",
            "reviews --> review\n",
            "The --> the\n",
            "infantile --> infantil\n",
            "cart --> cart\n",
            "is --> is\n",
            "simple --> simpl\n",
            "and --> and\n",
            "easy --> easi\n",
            "to --> to\n",
            "use --> use\n",
            "This --> this\n",
            "kind --> kind\n",
            "of --> of\n",
            "humour --> humour\n",
            "is --> is\n",
            "infantile --> infantil\n",
            "and --> and\n",
            "boring --> bore\n",
            "The --> the\n",
            "word --> word\n",
            "infantile --> infantil\n",
            "indicates --> indic\n",
            "negative --> negat\n",
            "sentiment --> sentiment\n",
            "in --> in\n",
            "Movie --> movi\n",
            "task --> task\n",
            "while --> while\n",
            "it --> it\n",
            "is --> is\n",
            "neutral --> neutral\n",
            "in --> in\n",
            "Baby --> babi\n",
            "task --> task\n",
            "However --> howev\n",
            "the --> the\n",
            "general --> general\n",
            "shared --> share\n",
            "private --> privat\n",
            "model --> model\n",
            "could --> could\n",
            "place --> place\n",
            "the --> the\n",
            "task --> task\n",
            "specific --> specif\n",
            "word --> word\n",
            "infantile --> infantil\n",
            "in --> in\n",
            "a --> a\n",
            "shared --> share\n",
            "space --> space\n",
            "leaving --> leav\n",
            "potential --> potenti\n",
            "hazards --> hazard\n",
            "for --> for\n",
            "other --> other\n",
            "tasks --> task\n",
            "Additionally --> addit\n",
            "the --> the\n",
            "capacity --> capac\n",
            "of --> of\n",
            "shared --> share\n",
            "space --> space\n",
            "could --> could\n",
            "also --> also\n",
            "be --> be\n",
            "wasted --> wast\n",
            "by --> by\n",
            "some --> some\n",
            "unnecessary --> unnecessari\n",
            "features --> featur\n",
            "To --> to\n",
            "address --> address\n",
            "this --> this\n",
            "problem --> problem\n",
            "in --> in\n",
            "this --> this\n",
            "paper --> paper\n",
            "we --> we\n",
            "propose --> propos\n",
            "an --> an\n",
            "adversarial --> adversari\n",
            "multi --> multi\n",
            "task --> task\n",
            "framework --> framework\n",
            "in --> in\n",
            "which --> which\n",
            "the --> the\n",
            "shared --> share\n",
            "and --> and\n",
            "private --> privat\n",
            "feature --> featur\n",
            "spaces --> space\n",
            "are --> are\n",
            "in --> in\n",
            "herently --> herent\n",
            "disjoint --> disjoint\n",
            "by --> by\n",
            "introducing --> introduc\n",
            "orthogonality --> orthogon\n",
            "constraints --> constraint\n",
            "Specifically --> specif\n",
            "we --> we\n",
            "design --> design\n",
            "a --> a\n",
            "generic --> generic\n",
            "shared --> share\n",
            "private --> privat\n",
            "learning --> learn\n",
            "framework --> framework\n",
            "to --> to\n",
            "model --> model\n",
            "the --> the\n",
            "text --> text\n",
            "sequence --> sequenc\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cb7rzvOhmbT"
      },
      "source": [
        "from gensim.utils import tokenize\n",
        "tokenize_text_gensim = tokenize(text)\n",
        "tokens = list(tokenize(text))\n",
        "with open('/content/reserch_paper_out_stem.txt', 'w') as writefile:\n",
        "    for token in tokens:\n",
        "      writefile.write(str(token + ' --> ' + stemmer.stem(token)))"
      ],
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmImcRyDl173"
      },
      "source": [
        "# spell correction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJ9WCy-9mB3T"
      },
      "source": [
        "**Isolated word correction**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MHz38aEhQtdP"
      },
      "source": [
        "pip install pyspellchecker"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJny_YLDbwHt"
      },
      "source": [
        "pip install symspellpy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4iHoFk0FQFPo"
      },
      "source": [
        "from spellchecker import SpellChecker\n",
        "\n",
        "spell = SpellChecker()\n",
        "def correct_spellings(text):\n",
        "    text = text\n",
        "    corrected_text = []\n",
        "    misspelled_words = spell.unknown(text.split())\n",
        "    for word in text.split():\n",
        "        if word in misspelled_words:\n",
        "            corrected_text.append(text + '  -->was changes as -->'+ spell.correction(word))\n",
        "        else:\n",
        "            continue\n",
        "    return \" \".join(corrected_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOeVxENDRCDb",
        "outputId": "f88f58e8-4d40-441c-c2c1-ceec60251f52",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "tokens = tokenize(text)\n",
        "l=[]\n",
        "for word in tokens:\n",
        "    l.append(correct_spellings(word))\n",
        "spel_changed_words_list = [i for i in l if i] \n",
        "spel_changed_words_list"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['sharable  -->was changes as -->shareable',\n",
              " 'herently  -->was changes as -->recently',\n",
              " 'orthogonality  -->was changes as -->orthogonality']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMisNCGzmLZ5"
      },
      "source": [
        "**Context Sensitive word correction**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ww7cQiTxbfgm",
        "outputId": "444437b1-e23e-49a5-c9b4-1544f58cd73f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from symspellpy.symspellpy import SymSpell, Verbosity\n",
        "word_length = 2\n",
        "prefix_length = 7\n",
        "sym_spell = SymSpell(word_length, prefix_length)\n",
        "print(\"Corpus file not found\") if not sym_spell.create_dictionary(\"/content/big.txt\") else print(\"Success!\")\n"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Success!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v7EIbLKwcF7r"
      },
      "source": [
        "preview = 10\n",
        "def correct_tocknized_text(words):\n",
        "    corr_count = 0\n",
        "    corrected_words = []\n",
        "    for i, word in enumerate(words[:-word_length+1]):\n",
        "        word_set = [words[i+j] for j in range(word_length)]\n",
        "        _input = ' '.join(word_set)\n",
        "        result = sym_spell.word_segmentation(_input)\n",
        "        correction = result.corrected_string\n",
        "        if correction.lower() != _input.lower() and preview < corr_count:\n",
        "            corr_count += 1\n",
        "            print('\"{}\" is corrected as \"{}\"'.format(_input, correction))\n",
        "        corrected_words.append(correction.split(' ')[0])\n",
        "    corrected_words.append(correction.split(' ')[1])\n",
        "    return corrected_words"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8CLZk7riX_x",
        "outputId": "b8b8f5e1-3cfa-4704-efab-b44abd63b6dc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(correct_tocknized_text(list(tokenize(text))))"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Neutral', 'network', 'models', 'have', 'shown', 'their', 'promising', 'opportunities', 'for', 'multi', 'task', 'learning', 'which', 'focus', 'on', 'learning', 'the', 'shared', 'layers', 'to', 'extract', 'the', 'common', 'and', 'task', 'in', 'features', 'However', 'in', 'most', 'existing', 'approaches', 'the', 'extracted', 'shared', 'features', 'are', 'prone', 'to', 'be', 'contaminated', 'by', 'task', 'specific', 'features', 'or', 'the', 'noise', 'brought', 'by', 'other', 'tasks', 'In', 'this', 'paper', 'we', 'propose', 'an', 'adversary', 'multi', 'task', 'learning', 'framework', 'alleviating', 'the', 'shared', 'and', 'private', 'latent', 'feature', 'spaces', 'from', 'interfering', 'with', 'each', 'other', 'We', 'conduct', 'extensive', 'experiments', 'on', 'different', 'text', 'classification', 'tasks', 'which', 'demonstrates', 'the', 'benefits', 'of', 'our', 'approach', 'Besides', 'we', 'show', 'that', 'the', 'shared', 'knowledge', 'learned', 'by', 'our', 'proposed', 'model', 'can', 'be', 'regarded', 'as', 'off', 'the', 'shelf', 'knowledge', 'and', 'easily', 'transferred', 'to', 'new', 'tasks', 'Multi', 'task', 'learning', 'is', 'an', 'effective', 'approach', 'to', 'improve', 'the', 'performance', 'of', 'a', 'single', 'task', 'with', 'the', 'help', 'of', 'other', 'related', 'tasks', 'Recently', 'neutral', 'based', 'models', 'for', 'multi', 'task', 'learning', 'have', 'become', 'very', 'popular', 'ranging', 'from', 'computer', 'vision', 'Misha', 'et', 'al', 'Hang', 'et', 'al', 'to', 'natural', 'language', 'processing', 'Follower', 'andwew', 'Long', 'et', 'al', 'since', 'they', 'provide', 'a', 'convenient', 'way', 'of', 'combining', 'information', 'from', 'multiple', 'tasks', 'However', 'most', 'existing', 'work', 'on', 'multi', 'task', 'learning', 'Lie', 'et', 'al', 'c', 'b', 'attempts', 'to', 'divide', 'the', 'features', 'of', 'different', 'tasks', 'into', 'private', 'and', 'shared', 'spaces', 'merely', 'based', 'on', 'whether', 'parameter', 'of', 'some', 'components', 'should', 'be', 'shared', 'As', 'shown', 'in', 'Figure', 'a', 'the', 'general', 'shared', 'private', 'model', 'introduces', 'two', 'feature', 'spaces', 'for', 'any', 'task', 'one', 'is', 'used', 'to', 'store', 'task', 'dependent', 'features', 'the', 'other', 'is', 'used', 'to', 'capture', 'shared', 'features', 'The', 'major', 'limitation', 'of', 'this', 'framework', 'is', 'that', 'the', 'shared', 'feature', 'space', 'could', 'contain', 'some', 'unnecessary', 'task', 'specific', 'features', 'while', 'some', 'arable', 'features', 'could', 'also', 'be', 'mixed', 'in', 'private', 'space', 'suffering', 'from', 'feature', 'redundancy', 'Taking', 'the', 'following', 'two', 'sentences', 'as', 'examples', 'which', 'are', 'extracted', 'from', 'two', 'different', 'sentiment', 'classification', 'tasks', 'Movie', 'reviews', 'and', 'Baby', 'products', 'reviews', 'The', 'infantile', 'cart', 'is', 'simple', 'and', 'easy', 'to', 'use', 'This', 'kind', 'of', 'humour', 'is', 'infantile', 'and', 'boring', 'The', 'word', 'infantile', 'indicates', 'negative', 'sentiment', 'in', 'Movie', 'task', 'while', 'it', 'is', 'neutral', 'in', 'Baby', 'task', 'However', 'the', 'general', 'shared', 'private', 'model', 'could', 'place', 'the', 'task', 'specific', 'word', 'infantile', 'in', 'a', 'shared', 'space', 'leaving', 'potential', 'hazards', 'for', 'other', 'tasks', 'Additional', 'the', 'capacity', 'of', 'shared', 'space', 'could', 'also', 'be', 'wasted', 'by', 'some', 'unnecessary', 'features', 'To', 'address', 'this', 'problem', 'in', 'this', 'paper', 'we', 'propose', 'an', 'adversary', 'multi', 'task', 'framework', 'in', 'which', 'the', 'shared', 'and', 'private', 'feature', 'spaces', 'are', 'inherently', 'recently', 'dis', 'by', 'introducing', 'orthodox', 'constraint', 'Specifically', 'we', 'design', 'a', 'genetic', 'shared', 'private', 'learning', 'framework', 'to', 'model', 'the', 'text', 'sequence']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JkNX00lmPfy"
      },
      "source": [
        "# lemmatize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNpGdogM-47s",
        "outputId": "43f4da64-3d9b-4c3b-e455-6345b4f8b3ba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "tokens = tokenize(text)\n",
        "for token in tokens:\n",
        "    print(token + ' --> ' + lemmatizer.lemmatize(token))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Neural --> Neural\n",
            "network --> network\n",
            "models --> model\n",
            "have --> have\n",
            "shown --> shown\n",
            "their --> their\n",
            "promising --> promising\n",
            "opportunities --> opportunity\n",
            "for --> for\n",
            "multi --> multi\n",
            "task --> task\n",
            "learning --> learning\n",
            "which --> which\n",
            "focus --> focus\n",
            "on --> on\n",
            "learning --> learning\n",
            "the --> the\n",
            "shared --> shared\n",
            "layers --> layer\n",
            "to --> to\n",
            "extract --> extract\n",
            "the --> the\n",
            "common --> common\n",
            "and --> and\n",
            "task --> task\n",
            "invariant --> invariant\n",
            "features --> feature\n",
            "However --> However\n",
            "in --> in\n",
            "most --> most\n",
            "existing --> existing\n",
            "approaches --> approach\n",
            "the --> the\n",
            "extracted --> extracted\n",
            "shared --> shared\n",
            "features --> feature\n",
            "are --> are\n",
            "prone --> prone\n",
            "to --> to\n",
            "be --> be\n",
            "contaminated --> contaminated\n",
            "by --> by\n",
            "task --> task\n",
            "specific --> specific\n",
            "features --> feature\n",
            "or --> or\n",
            "the --> the\n",
            "noise --> noise\n",
            "brought --> brought\n",
            "by --> by\n",
            "other --> other\n",
            "tasks --> task\n",
            "In --> In\n",
            "this --> this\n",
            "paper --> paper\n",
            "we --> we\n",
            "propose --> propose\n",
            "an --> an\n",
            "adversarial --> adversarial\n",
            "multi --> multi\n",
            "task --> task\n",
            "learning --> learning\n",
            "framework --> framework\n",
            "alleviating --> alleviating\n",
            "the --> the\n",
            "shared --> shared\n",
            "and --> and\n",
            "private --> private\n",
            "latent --> latent\n",
            "feature --> feature\n",
            "spaces --> space\n",
            "from --> from\n",
            "interfering --> interfering\n",
            "with --> with\n",
            "each --> each\n",
            "other --> other\n",
            "We --> We\n",
            "conduct --> conduct\n",
            "extensive --> extensive\n",
            "experiments --> experiment\n",
            "on --> on\n",
            "different --> different\n",
            "text --> text\n",
            "classification --> classification\n",
            "tasks --> task\n",
            "which --> which\n",
            "demonstrates --> demonstrates\n",
            "the --> the\n",
            "benefits --> benefit\n",
            "of --> of\n",
            "our --> our\n",
            "approach --> approach\n",
            "Besides --> Besides\n",
            "we --> we\n",
            "show --> show\n",
            "that --> that\n",
            "the --> the\n",
            "shared --> shared\n",
            "knowledge --> knowledge\n",
            "learned --> learned\n",
            "by --> by\n",
            "our --> our\n",
            "proposed --> proposed\n",
            "model --> model\n",
            "can --> can\n",
            "be --> be\n",
            "regarded --> regarded\n",
            "as --> a\n",
            "off --> off\n",
            "the --> the\n",
            "shelf --> shelf\n",
            "knowledge --> knowledge\n",
            "and --> and\n",
            "easily --> easily\n",
            "transferred --> transferred\n",
            "to --> to\n",
            "new --> new\n",
            "tasks --> task\n",
            "Multi --> Multi\n",
            "task --> task\n",
            "learning --> learning\n",
            "is --> is\n",
            "an --> an\n",
            "effective --> effective\n",
            "approach --> approach\n",
            "to --> to\n",
            "improve --> improve\n",
            "the --> the\n",
            "performance --> performance\n",
            "of --> of\n",
            "a --> a\n",
            "single --> single\n",
            "task --> task\n",
            "with --> with\n",
            "the --> the\n",
            "help --> help\n",
            "of --> of\n",
            "other --> other\n",
            "related --> related\n",
            "tasks --> task\n",
            "Recently --> Recently\n",
            "neural --> neural\n",
            "based --> based\n",
            "models --> model\n",
            "for --> for\n",
            "multi --> multi\n",
            "task --> task\n",
            "learning --> learning\n",
            "have --> have\n",
            "become --> become\n",
            "very --> very\n",
            "popular --> popular\n",
            "ranging --> ranging\n",
            "from --> from\n",
            "computer --> computer\n",
            "vision --> vision\n",
            "Misra --> Misra\n",
            "et --> et\n",
            "al --> al\n",
            "Zhang --> Zhang\n",
            "et --> et\n",
            "al --> al\n",
            "to --> to\n",
            "natural --> natural\n",
            "language --> language\n",
            "processing --> processing\n",
            "Collobert --> Collobert\n",
            "andWeston --> andWeston\n",
            "Luong --> Luong\n",
            "et --> et\n",
            "al --> al\n",
            "since --> since\n",
            "they --> they\n",
            "provide --> provide\n",
            "a --> a\n",
            "convenient --> convenient\n",
            "way --> way\n",
            "of --> of\n",
            "combining --> combining\n",
            "information --> information\n",
            "from --> from\n",
            "multiple --> multiple\n",
            "tasks --> task\n",
            "However --> However\n",
            "most --> most\n",
            "existing --> existing\n",
            "work --> work\n",
            "on --> on\n",
            "multi --> multi\n",
            "task --> task\n",
            "learning --> learning\n",
            "Liu --> Liu\n",
            "et --> et\n",
            "al --> al\n",
            "c --> c\n",
            "b --> b\n",
            "attempts --> attempt\n",
            "to --> to\n",
            "divide --> divide\n",
            "the --> the\n",
            "features --> feature\n",
            "of --> of\n",
            "different --> different\n",
            "tasks --> task\n",
            "into --> into\n",
            "private --> private\n",
            "and --> and\n",
            "shared --> shared\n",
            "spaces --> space\n",
            "merely --> merely\n",
            "based --> based\n",
            "on --> on\n",
            "whether --> whether\n",
            "parameters --> parameter\n",
            "of --> of\n",
            "some --> some\n",
            "components --> component\n",
            "should --> should\n",
            "be --> be\n",
            "shared --> shared\n",
            "As --> As\n",
            "shown --> shown\n",
            "in --> in\n",
            "Figure --> Figure\n",
            "a --> a\n",
            "the --> the\n",
            "general --> general\n",
            "shared --> shared\n",
            "private --> private\n",
            "model --> model\n",
            "introduces --> introduces\n",
            "two --> two\n",
            "feature --> feature\n",
            "spaces --> space\n",
            "for --> for\n",
            "any --> any\n",
            "task --> task\n",
            "one --> one\n",
            "is --> is\n",
            "used --> used\n",
            "to --> to\n",
            "store --> store\n",
            "task --> task\n",
            "dependent --> dependent\n",
            "features --> feature\n",
            "the --> the\n",
            "other --> other\n",
            "is --> is\n",
            "used --> used\n",
            "to --> to\n",
            "capture --> capture\n",
            "shared --> shared\n",
            "features --> feature\n",
            "The --> The\n",
            "major --> major\n",
            "limitation --> limitation\n",
            "of --> of\n",
            "this --> this\n",
            "framework --> framework\n",
            "is --> is\n",
            "that --> that\n",
            "the --> the\n",
            "shared --> shared\n",
            "feature --> feature\n",
            "space --> space\n",
            "could --> could\n",
            "contain --> contain\n",
            "some --> some\n",
            "unnecessary --> unnecessary\n",
            "task --> task\n",
            "specific --> specific\n",
            "features --> feature\n",
            "while --> while\n",
            "some --> some\n",
            "sharable --> sharable\n",
            "features --> feature\n",
            "could --> could\n",
            "also --> also\n",
            "be --> be\n",
            "mixed --> mixed\n",
            "in --> in\n",
            "private --> private\n",
            "space --> space\n",
            "suffering --> suffering\n",
            "from --> from\n",
            "feature --> feature\n",
            "redundancy --> redundancy\n",
            "Taking --> Taking\n",
            "the --> the\n",
            "following --> following\n",
            "two --> two\n",
            "sentences --> sentence\n",
            "as --> a\n",
            "examples --> example\n",
            "which --> which\n",
            "are --> are\n",
            "extracted --> extracted\n",
            "from --> from\n",
            "two --> two\n",
            "different --> different\n",
            "sentiment --> sentiment\n",
            "classification --> classification\n",
            "tasks --> task\n",
            "Movie --> Movie\n",
            "reviews --> review\n",
            "and --> and\n",
            "Baby --> Baby\n",
            "products --> product\n",
            "reviews --> review\n",
            "The --> The\n",
            "infantile --> infantile\n",
            "cart --> cart\n",
            "is --> is\n",
            "simple --> simple\n",
            "and --> and\n",
            "easy --> easy\n",
            "to --> to\n",
            "use --> use\n",
            "This --> This\n",
            "kind --> kind\n",
            "of --> of\n",
            "humour --> humour\n",
            "is --> is\n",
            "infantile --> infantile\n",
            "and --> and\n",
            "boring --> boring\n",
            "The --> The\n",
            "word --> word\n",
            "infantile --> infantile\n",
            "indicates --> indicates\n",
            "negative --> negative\n",
            "sentiment --> sentiment\n",
            "in --> in\n",
            "Movie --> Movie\n",
            "task --> task\n",
            "while --> while\n",
            "it --> it\n",
            "is --> is\n",
            "neutral --> neutral\n",
            "in --> in\n",
            "Baby --> Baby\n",
            "task --> task\n",
            "However --> However\n",
            "the --> the\n",
            "general --> general\n",
            "shared --> shared\n",
            "private --> private\n",
            "model --> model\n",
            "could --> could\n",
            "place --> place\n",
            "the --> the\n",
            "task --> task\n",
            "specific --> specific\n",
            "word --> word\n",
            "infantile --> infantile\n",
            "in --> in\n",
            "a --> a\n",
            "shared --> shared\n",
            "space --> space\n",
            "leaving --> leaving\n",
            "potential --> potential\n",
            "hazards --> hazard\n",
            "for --> for\n",
            "other --> other\n",
            "tasks --> task\n",
            "Additionally --> Additionally\n",
            "the --> the\n",
            "capacity --> capacity\n",
            "of --> of\n",
            "shared --> shared\n",
            "space --> space\n",
            "could --> could\n",
            "also --> also\n",
            "be --> be\n",
            "wasted --> wasted\n",
            "by --> by\n",
            "some --> some\n",
            "unnecessary --> unnecessary\n",
            "features --> feature\n",
            "To --> To\n",
            "address --> address\n",
            "this --> this\n",
            "problem --> problem\n",
            "in --> in\n",
            "this --> this\n",
            "paper --> paper\n",
            "we --> we\n",
            "propose --> propose\n",
            "an --> an\n",
            "adversarial --> adversarial\n",
            "multi --> multi\n",
            "task --> task\n",
            "framework --> framework\n",
            "in --> in\n",
            "which --> which\n",
            "the --> the\n",
            "shared --> shared\n",
            "and --> and\n",
            "private --> private\n",
            "feature --> feature\n",
            "spaces --> space\n",
            "are --> are\n",
            "in --> in\n",
            "herently --> herently\n",
            "disjoint --> disjoint\n",
            "by --> by\n",
            "introducing --> introducing\n",
            "orthogonality --> orthogonality\n",
            "constraints --> constraint\n",
            "Specifically --> Specifically\n",
            "we --> we\n",
            "design --> design\n",
            "a --> a\n",
            "generic --> generic\n",
            "shared --> shared\n",
            "private --> private\n",
            "learning --> learning\n",
            "framework --> framework\n",
            "to --> to\n",
            "model --> model\n",
            "the --> the\n",
            "text --> text\n",
            "sequence --> sequence\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWrKLeehiETT"
      },
      "source": [
        "from gensim.utils import tokenize\n",
        "tokenize_text_gensim = tokenize(text)\n",
        "tokens = list(tokenize(text))\n",
        "with open('/content/reserch_paper_out_lematize.txt', 'w') as writefile:\n",
        "    for token in tokens:\n",
        "      writefile.write(str(token + ' --> ' + lemmatizer.lemmatize(token)))"
      ],
      "execution_count": 108,
      "outputs": []
    }
  ]
}